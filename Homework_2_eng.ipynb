{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Base on  [BobaZooba hw](https://github.com/BobaZooba/DeepNLP/blob/2020/Week%203/Homework%202.ipynb)"
      ],
      "metadata": {
        "id": "zg4SNPVSp4II"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_n88FxPp0HN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "\n",
        "import zipfile\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from data import Downloader, Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bFT_aogp0HO"
      },
      "source": [
        "### Loading the file with embeddings for English\n",
        "We will need them a little later.\n",
        "\n",
        "For other languages: https://fasttext.cc/docs/en/crawl-vectors.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek14oFkpp0HP",
        "outputId": "2042a70e-3a77-48ee-ae39-1000c5e8448b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2020-10-05 06:56:55--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
            "Распознаётся dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)… 172.67.9.4, 104.22.74.142, 104.22.75.142\n",
            "Подключение к dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... соединение установлено.\n",
            "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
            "Длина: 681808098 (650M) [application/zip]\n",
            "Сохранение в: «wiki-news-300d-1M.vec.zip»\n",
            "\n",
            "wiki-news-300d-1M.v 100%[===================>] 650,22M  9,35MB/s    за 71s     \n",
            "\n",
            "2020-10-05 06:58:08 (9,13 MB/s) - «wiki-news-300d-1M.vec.zip» сохранён [681808098/681808098]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# uncomment and download\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sCNtHiSp0HQ"
      },
      "outputs": [],
      "source": [
        "# path to data\n",
        "data_path = './data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia_C6wVNp0HQ"
      },
      "source": [
        "### Data reader\n",
        "No need to go into details, this thing just downloads data, then parses it and makes three datasets from it:\n",
        "- training\n",
        "- validation\n",
        "- unlabeled\n",
        "\n",
        "Unlabeled data is not essential, but you may need it, for example, for a language model or to improve embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meDnkjHGp0HQ"
      },
      "outputs": [],
      "source": [
        "downloader = Downloader(data_path=data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcZ7av6bp0HQ",
        "outputId": "1c992dcc-7bf1-44f6-c27f-f957e3540cc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "single: 100%|██████████| 21/21 [02:18<00:00,  6.60s/it]\n",
            "multiple: 100%|██████████| 17/17 [03:46<00:00, 13.32s/it]\n"
          ]
        }
      ],
      "source": [
        "downloader.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fis3sv-dp0HR"
      },
      "outputs": [],
      "source": [
        "parser = Parser(data_path=data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KozrrLp2p0HR"
      },
      "outputs": [],
      "source": [
        "unlabeled, train, valid = parser.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im3PZIj7p0HR"
      },
      "source": [
        "### Let's look at the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCMUqtrYp0HS"
      },
      "outputs": [],
      "source": [
        "unlabeled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga-y-PQcp0HS"
      },
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXOhZ3EEp0HS"
      },
      "outputs": [],
      "source": [
        "valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQELow3qp0HS"
      },
      "source": [
        "## Task\n",
        "Classify the question field into one of the categories in the category field.\n",
        "\n",
        "This is data from the Amazon QA service, that is, a service where you can ask a question and get an answer from other users.\n",
        "\n",
        "The idea of ​​the task is the following: let's help the client determine which category to post his question to in order to quickly get the most relevant answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLQpPnVvp0HS"
      },
      "source": [
        "### Converting a class into an index\n",
        "We will code some mapper that converts the class text into a specific unique index. We need this because our model does not work directly with the class, but with its index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv62gHzop0HS"
      },
      "outputs": [],
      "source": [
        "# checking that the train and the validation datasets contain the same categories\n",
        "set(train.category.unique().tolist()) == set(valid.category.unique().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss2SiLgCp0HT"
      },
      "outputs": [],
      "source": [
        "unique_categories = set(train.category.unique().tolist() + valid.category.unique().tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKvqnML8p0HT"
      },
      "outputs": [],
      "source": [
        "category2index = {category: index for index, category in enumerate(unique_categories)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtPRHwVAp0HT"
      },
      "outputs": [],
      "source": [
        "category2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9OQb4COp0HU"
      },
      "outputs": [],
      "source": [
        "train['target'] = train.category.map(category2index)\n",
        "valid['target'] = valid.category.map(category2index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhTpK-cZp0HU"
      },
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOAWUz_7p0HU"
      },
      "source": [
        "### Torch Dataset, DataLoader\n",
        "\n",
        "This is a very important abstraction for Torch.\n",
        "\n",
        "We will always use it to work with data.\n",
        "\n",
        "`Dataset` is a class that you need to inherit from to write your own data handler. Inside it, you need to implement two methods,\n",
        "which will be discussed below. That is, in this class you describe how to convert your data into a Torch format (converting texts\n",
        "into word indexes, etc.).\n",
        "\n",
        "`DataLoader` is a class that will sample data in batches for you. It is an iterator, so the format for working with it is approximately as follows:\n",
        "```python\n",
        "for batch in data_loader:\n",
        "    ...\n",
        "```\n",
        "That is, at each iteration, one batch of data is given. Iteration ends when you go through all the batches.\n",
        "\n",
        "Why do we need these abstractions? To simplify and unify our work with data.\n",
        "In general, you can implement something of your own, but this is a simplification of this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuBX0qTRp0HU"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtNpYACWp0HV"
      },
      "outputs": [],
      "source": [
        "# toy dataset\n",
        "# 121535 examples, 4 features, 3 classes\n",
        "some_data_x = np.random.rand(121535, 4)\n",
        "some_data_y = np.random.randint(3, size=(121535,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AEjK6A9p0HV"
      },
      "outputs": [],
      "source": [
        "# just random numbers\n",
        "some_data_x[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTdPP8U0p0HV"
      },
      "outputs": [],
      "source": [
        "# and classes\n",
        "some_data_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCoMD9Ccp0HV"
      },
      "source": [
        "### Example of usefulness\n",
        "To train a model, you need to feed it batches of data. How could we implement this if we didn't have Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7gaogQCp0HV"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "for i_batch in range(math.ceil(some_data_x.shape[0] / batch_size)):\n",
        "\n",
        "    x_batch = some_data_x[i_batch * batch_size:(i_batch + 1) * batch_size]\n",
        "    y_batch = some_data_y[i_batch * batch_size:(i_batch + 1) * batch_size]\n",
        "\n",
        "    x_batch = torch.tensor(x_batch)\n",
        "    y_batch = torch.tensor(y_batch)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpmsEYxFp0HW"
      },
      "outputs": [],
      "source": [
        "x_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HslxQoKzp0HW"
      },
      "outputs": [],
      "source": [
        "x_batch.shape, y_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKtbXt_Bp0HW"
      },
      "source": [
        "This is a fairly simple example. We were able to do it ourselves, but almost always, processing the data to feed it into a model is more complicated.\n",
        "And some things are often needed more than once, for example, if we want to shuffle our data every epoch to get different batches.\n",
        "We can do this, but to do so, we will have to drag some code with us from project to project. Also, co-development or simply reading someone else's code is much easier when you use unified formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mccmAkLBp0HW"
      },
      "source": [
        "### Moving on to Dataset\n",
        "Let's wrap our data in this handler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiUD-Ugfp0HW"
      },
      "outputs": [],
      "source": [
        "class ToyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_x, data_y):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_x = data_x\n",
        "        self.data_y = data_y\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # it is very necessary to define this function\n",
        "        # it should return the size of the dataset\n",
        "        # it is needed for DataLoader to sample batches\n",
        "\n",
        "        return len(self.data_x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # this method needs to be defined as well\n",
        "        # that is, how we will get our data by index\n",
        "\n",
        "        return self.data_x[idx], self.data_y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkHSn_Cap0HW"
      },
      "outputs": [],
      "source": [
        "some_dataset = ToyDataset(some_data_x, some_data_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsTzxqjvp0HW"
      },
      "outputs": [],
      "source": [
        "some_dataset[5], some_dataset[467]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USLWDrm5p0HW"
      },
      "source": [
        "### It seems like it doesn't make sense, but this is the simplest example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOa2EaeJp0HW"
      },
      "source": [
        "### DataLoader\n",
        "We can set some parameters in it, for example, batch size and whether it is necessary to shuffle data in every pass to get different batches (to compose these batches differently)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpScFxLbp0HX"
      },
      "outputs": [],
      "source": [
        "some_loader = DataLoader(some_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi-Q6rLip0HX"
      },
      "outputs": [],
      "source": [
        "for x, y in some_loader:\n",
        "    break\n",
        "\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMj088-Pp0HX"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebGFNrX1p0HX"
      },
      "outputs": [],
      "source": [
        "for x, y in some_loader:\n",
        "    pass\n",
        "\n",
        "len(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er7UQx_Fp0HX"
      },
      "outputs": [],
      "source": [
        "# why 15?\n",
        "# because the amount of our data is not divisible by 16\n",
        "# and therefore the last batch is less than 16\n",
        "len(some_dataset) % 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsxvVXjCp0HX"
      },
      "source": [
        "### Let's complicate the handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yRnSZHhp0HX"
      },
      "outputs": [],
      "source": [
        "class ToyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_x, data_y):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_x = data_x\n",
        "        self.data_y = data_y\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # it is very necessary to define this function\n",
        "        # it should return the size of the dataset\n",
        "        # it is needed for DataLoader to sample batches\n",
        "\n",
        "        return len(self.data_x)\n",
        "\n",
        "    @staticmethod\n",
        "    def pow_features(x, n=2):\n",
        "\n",
        "        return x ** n\n",
        "\n",
        "    @staticmethod\n",
        "    def log_features(x):\n",
        "\n",
        "        return np.log(x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # this method needs to be defined as well\n",
        "        # that is, how we will get our data by index\n",
        "\n",
        "        x = self.data_x[idx]\n",
        "\n",
        "        # inside the dataset we can do whatever we want with our data\n",
        "        # for example, to define functions that add power features\n",
        "        x_p_2 = self.pow_features(x, n=2)\n",
        "        x_p_3 = self.pow_features(x, n=3)\n",
        "        # and let's also add logarithmic features\n",
        "        x_log = self.log_features(x)\n",
        "\n",
        "        # let's concatenate our features\n",
        "        x = np.concatenate([x, x_p_2, x_p_3, x_log])\n",
        "\n",
        "        y = self.data_y[idx]\n",
        "\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExndjhSBp0HY"
      },
      "outputs": [],
      "source": [
        "toy_dataset = ToyDataset(some_data_x, some_data_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_tDko7Vp0HY"
      },
      "outputs": [],
      "source": [
        "toy_loader = DataLoader(dataset=toy_dataset, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31TUGLClp0HY"
      },
      "outputs": [],
      "source": [
        "for x, y in toy_loader:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVArA8u7p0HY"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruyBkvFyp0HY"
      },
      "outputs": [],
      "source": [
        "# note that we immediately get the torch data format, which is obtained from the automatic conversion from numpy\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPKKn-wVp0HY"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL7uzHkJp0HY"
      },
      "outputs": [],
      "source": [
        "# let's create a small model and calculate the loss\n",
        "\n",
        "model = torch.nn.Sequential(torch.nn.Linear(16, 8),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(8, 4),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Linear(4, 3))\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    prediction = model(x.float())\n",
        "\n",
        "    loss = criterion(prediction, y)\n",
        "\n",
        "loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hWvXoKzp0HY"
      },
      "source": [
        "### Let's create a dataset for our text data\n",
        "We will input a string and a target by index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHtcNdp0p0HZ"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, texts, targets):\n",
        "        super().__init__()\n",
        "\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        text = self.texts[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        return text, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cBeOYScp0HZ"
      },
      "outputs": [],
      "source": [
        "# preparing the data\n",
        "train_x = list(train.question)\n",
        "train_y = list(train.target)\n",
        "\n",
        "valid_x = list(valid.question)\n",
        "valid_y = list(valid.target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opHf1NUCp0HZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(texts=list(train.question), targets=list(train.target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPeFjIsOp0HZ"
      },
      "outputs": [],
      "source": [
        "# sampling the data\n",
        "text, target = train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kFX2-8Rp0HZ"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLfpktO8p0HZ"
      },
      "outputs": [],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVr42ap7p0HZ"
      },
      "source": [
        "### The point of the handler\n",
        "It is that we need to transform our data into a format that we can then pass to the model.\n",
        "Right now we have strings, and Torch doesn't know anything about strings, it needs tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjVxlmfPp0HZ"
      },
      "source": [
        "### Loading Embeddings\n",
        "To work with text data, we can split our lines into words, and convert the words into vectors. Where do we get these vectors?\n",
        "We talked about a method called word2vec and at the beginning of this notebook we loaded a file with these very vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2R1WhlTp0HZ"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYrw35whp0HZ"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(zip_path, filename, pad_token='PAD', max_words=100_000, verbose=True):\n",
        "\n",
        "    vocab = dict()\n",
        "    embeddings = list()\n",
        "\n",
        "    with zipfile.ZipFile(zip_path) as zipped_file:\n",
        "        with zipped_file.open(filename) as file_object:\n",
        "\n",
        "            vocab_size, embedding_dim = file_object.readline().decode('utf-8').strip().split()\n",
        "\n",
        "            vocab_size = int(vocab_size)\n",
        "            embedding_dim = int(embedding_dim)\n",
        "\n",
        "            # there are 1,000,000 words with vectors in the file, let's limit this dictionary for simplicity\n",
        "            max_words = vocab_size if max_words <= 0 else max_words\n",
        "\n",
        "            # let's add the pad token and embedding to our embedding matrix and dictionary\n",
        "            vocab[pad_token] = len(vocab)\n",
        "            embeddings.append(np.zeros(embedding_dim))\n",
        "\n",
        "            progress_bar = tqdm(total=max_words, disable=not verbose)\n",
        "\n",
        "            for line in file_object:\n",
        "                parts = line.decode('utf-8').strip().split()\n",
        "\n",
        "                token = ' '.join(parts[:-embedding_dim]).lower()\n",
        "\n",
        "                if token in vocab:\n",
        "                    continue\n",
        "\n",
        "                word_vector = np.array(list(map(float, parts[-embedding_dim:])))\n",
        "\n",
        "                vocab[token] = len(vocab)\n",
        "                embeddings.append(word_vector)\n",
        "\n",
        "                progress_bar.update()\n",
        "\n",
        "                if len(vocab) == max_words:\n",
        "                    break\n",
        "\n",
        "            progress_bar.close()\n",
        "\n",
        "    embeddings = np.stack(embeddings)\n",
        "\n",
        "    return vocab, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "6Y3U3W_Mp0Ha"
      },
      "outputs": [],
      "source": [
        "vocab, embeddings = load_embeddings('./wiki-news-300d-1M.vec.zip', 'wiki-news-300d-1M.vec', max_words=100_000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa5fZUAgp0Ha"
      },
      "source": [
        "### Let's look at the word's closest neighbors by embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLJozhsqp0Ha"
      },
      "outputs": [],
      "source": [
        "index2token = {index: token for token, index in vocab.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGnZAB57p0Ha"
      },
      "outputs": [],
      "source": [
        "emb_norms = np.linalg.norm(embeddings, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njZRsqbOp0Ha"
      },
      "outputs": [],
      "source": [
        "def get_k_nearest_neighbors(word, embeddings, emb_norms, vocab, index2token, k=5):\n",
        "\n",
        "    if word not in vocab:\n",
        "        print('Not in vocab')\n",
        "        return\n",
        "\n",
        "    word_index = vocab[word]\n",
        "\n",
        "    word_vector = embeddings[word_index]\n",
        "    word_vector = np.expand_dims(word_vector, 0)\n",
        "\n",
        "    scores = (word_vector @ embeddings.T)[0]\n",
        "\n",
        "    # convert to cosines, dividing by vector norms\n",
        "    # epsilon 1e-6 so as not to divide by 0\n",
        "    scores = scores / (emb_norms + 1e-6) / emb_norms[word_index]\n",
        "\n",
        "    # 1:k+1 because 0-indexed element is the word itself\n",
        "    for idx in scores.argsort()[::-1][1:k+1]:\n",
        "        print(f'The word {index2token[idx]} is similar by {scores[idx]:.2f} to the word {word}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef54bCSOp0Ha"
      },
      "outputs": [],
      "source": [
        "get_k_nearest_neighbors('anna', embeddings, emb_norms, vocab, index2token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyowgPaKp0Ha"
      },
      "outputs": [],
      "source": [
        "get_k_nearest_neighbors('mom', embeddings, emb_norms, vocab, index2token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elewn4qQp0Ha"
      },
      "outputs": [],
      "source": [
        "get_k_nearest_neighbors('have', embeddings, emb_norms, vocab, index2token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuOTMg5jp0Hb"
      },
      "outputs": [],
      "source": [
        "get_k_nearest_neighbors('money', embeddings, emb_norms, vocab, index2token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7tnc8HRp0Hb"
      },
      "outputs": [],
      "source": [
        "get_k_nearest_neighbors('music', embeddings, emb_norms, vocab, index2token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFiMGJaTp0Hb"
      },
      "source": [
        "### Choosing a tokenization method\n",
        "We now have a mapping that a certain word corresponds to a certain embedding of this word.\n",
        "Tokenization is the process of dividing a text into tokens, that is, parts of this text.\n",
        "How a \"word\" differs from a \"token\": a token is a more generalized concept, that is, for example, a number is a token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyZsKJP3p0Hb"
      },
      "outputs": [],
      "source": [
        "# More details about the differences can be found, for example, here\n",
        "# https://stackoverflow.com/questions/50240029/nltk-wordpunct-tokenize-vs-word-tokenize\n",
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9wxJoG4p0Hb"
      },
      "outputs": [],
      "source": [
        "total_n_words = 0\n",
        "unknown_words = list()\n",
        "\n",
        "for sample in tqdm(train_x):\n",
        "    # tokenization by space\n",
        "    tokens = sample.split()\n",
        "\n",
        "    for tok in tokens:\n",
        "        # checking if the token is in our dictionary\n",
        "        if tok not in vocab:\n",
        "            unknown_words.append(tok)\n",
        "\n",
        "        total_n_words += 1\n",
        "\n",
        "print(f'We don not know {len(unknown_words)} words out of {total_n_words} words in the dataset')\n",
        "print(f'Which is {len(unknown_words) * 100 / total_n_words:.2f}% of the dataset')\n",
        "print()\n",
        "print(f'Unique unknown words: {len(set(unknown_words))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kec5NFJ9p0Hb"
      },
      "outputs": [],
      "source": [
        "total_n_words = 0\n",
        "unknown_words = list()\n",
        "\n",
        "for sample in tqdm(train_x):\n",
        "    # tokenization\n",
        "    tokens = wordpunct_tokenize(sample)\n",
        "\n",
        "    for tok in tokens:\n",
        "        # checking if the token is in our dictionary\n",
        "        if tok not in vocab:\n",
        "            unknown_words.append(tok)\n",
        "\n",
        "        total_n_words += 1\n",
        "\n",
        "print(f'we don not know {len(unknown_words)} words out of {total_n_words} words in the dataset')\n",
        "print(f'Which is {len(unknown_words) * 100 / total_n_words:.2f}% of the dataset')\n",
        "print()\n",
        "print(f'Unique unknown words: {len(set(unknown_words))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-di5Ahpp0Hb"
      },
      "outputs": [],
      "source": [
        "total_n_words = 0\n",
        "unknown_words = list()\n",
        "\n",
        "for sample in tqdm(train_x):\n",
        "    # tokenization\n",
        "    tokens = word_tokenize(sample)\n",
        "\n",
        "    for tok in tokens:\n",
        "        # checking if the token is in our dictionary\n",
        "        if tok not in vocab:\n",
        "            unknown_words.append(tok)\n",
        "\n",
        "        total_n_words += 1\n",
        "\n",
        "print(f'we don not know {len(unknown_words)} words out of {total_n_words} words in the dataset')\n",
        "print(f'Which is {len(unknown_words) * 100 / total_n_words:.2f}% of the dataset')\n",
        "print()\n",
        "print(f'Unique unknown words: {len(set(unknown_words))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r48Bee3xp0Hb"
      },
      "source": [
        "### Results\n",
        "- The speed of word_tokenize is much lower than that of wordpunct_tokenize\n",
        "- Using word_tokenize, we lose about 1% of the information from the dataset compared to wordpunct_tokenize\n",
        "\n",
        "### The choice is obvious in favor of wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoTdio7np0Hb"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, texts, targets, vocab):\n",
        "        super().__init__()\n",
        "\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def tokenization(self, text):\n",
        "\n",
        "        tokens = wordpunct_tokenize(text)\n",
        "\n",
        "        token_indices = [self.vocab[tok] for tok in tokens if tok in self.vocab]\n",
        "\n",
        "        return token_indices\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        text = self.texts[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        tokenized_text = self.tokenization(text)\n",
        "\n",
        "        # let's translate our token indices into a Torch tensor\n",
        "        # the target will convert itself\n",
        "        tokenized_text = torch.tensor(tokenized_text)\n",
        "\n",
        "        return tokenized_text, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqw8crhyp0Hc"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(texts=train_x, targets=train_y, vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmhSWQAnp0Hc"
      },
      "outputs": [],
      "source": [
        "x, y = train_dataset[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLVMXlD5p0Hc"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFOB_xrWp0Hc"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsV-Hvfmp0Hc"
      },
      "outputs": [],
      "source": [
        "# we can restore the text back by word indexes\n",
        "[index2token[idx.item()] for idx in x]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5emZf5gKp0Hc"
      },
      "source": [
        "### У нас остается проблема разных длин текстов\n",
        "Чтобы поместить батч текстов в один тензор нам нужны одинаковые длины"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoGLGVHPp0Hc"
      },
      "outputs": [],
      "source": [
        "## this won't work, you can uncomment and check\n",
        "\n",
        "# x = [\n",
        "#     [1, 2, 3],\n",
        "#     [1, 2, 3, 4, 5],\n",
        "#     [1, 2, 3, 4, 5, 6, 7]\n",
        "# ]\n",
        "\n",
        "# torch.tensor(x), torch.tensor(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqvHRgpFp0Hc"
      },
      "outputs": [],
      "source": [
        "# this will work\n",
        "\n",
        "x = [\n",
        "    [1, 2, 3, 0, 0, 0, 0],\n",
        "    [1, 2, 3, 4, 5, 0, 0],\n",
        "    [1, 2, 3, 4, 5, 6, 7]\n",
        "]\n",
        "\n",
        "torch.tensor(x), torch.tensor(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVpPMw2Ap0Hd"
      },
      "source": [
        "### Text length\n",
        "We need to understand to what length we should pad each of our examples.\n",
        "We can find the maximum length of an example in tokens in our data and pad to this length, but this approach has a downside:\n",
        "we may have several texts with an abnormally large length, that is, some outliers.\n",
        "\n",
        "In this case, it is easier for us to limit the length of these texts to a certain statistic for our dataset. For example, 95% of our texts\n",
        "have a length of 25 words and this is enough for us. That is, we will limit the texts to this length, because almost the entire dataset fits within this length and we will not need to pad to a large length.\n",
        "\n",
        "We need padding so that we can place different examples in one batch, but we do not want to take these tokens into account. In fact, these will be idle runs and due to this compromise that most of the dataset is no more than n words and we can optimize our training.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "> Why don't we just throw away these long texts?\n",
        "\n",
        "The point is that we want to come to some compromise between the maximum length and the loss of information. If we take the 95th percentile of our lengths (that is, 95% of our texts are no larger than n), then throwing away the remaining 5%, we will lose a significant part of the examples.\n",
        "On the other hand, it may seem wrong to limit the length and this can really break the meaning of the examples, but this is often neglected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiOz4F1Rp0Hd"
      },
      "outputs": [],
      "source": [
        "train_lengths = [len(wordpunct_tokenize(sample)) for sample in tqdm(train_x)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3hU1Q08p0Hd"
      },
      "outputs": [],
      "source": [
        "sns.distplot(train_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBh49mEWp0Hd"
      },
      "outputs": [],
      "source": [
        "# we see large outliers in the data\n",
        "# 97% of our texts are no more than this many tokens\n",
        "np.percentile(train_lengths, 95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9iyFrUqp0Hd"
      },
      "outputs": [],
      "source": [
        "class TextClassificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, texts, targets, vocab, pad_index=0, max_length=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.pad_index = pad_index\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def tokenization(self, text):\n",
        "\n",
        "        tokens = wordpunct_tokenize(text)\n",
        "\n",
        "        token_indices = [self.vocab[tok] for tok in tokens if tok in self.vocab]\n",
        "\n",
        "        return token_indices\n",
        "\n",
        "    def padding(self, tokenized_text):\n",
        "\n",
        "        tokenized_text = tokenized_text[:self.max_length]\n",
        "\n",
        "        tokenized_text += [self.pad_index] * (self.max_length - len(tokenized_text))\n",
        "\n",
        "        return tokenized_text\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        text = self.texts[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        tokenized_text = self.tokenization(text)\n",
        "        tokenized_text = self.padding(tokenized_text)\n",
        "\n",
        "        tokenized_text = torch.tensor(tokenized_text)\n",
        "\n",
        "        return tokenized_text, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5FwsK_Pp0Hd"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(texts=train_x, targets=train_y, vocab=vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPxc8vN3p0Hd"
      },
      "outputs": [],
      "source": [
        "x, y = train_dataset[0]\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueIkgVZ3p0Hd"
      },
      "outputs": [],
      "source": [
        "[index2token[idx.item()] for idx in x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1k6FFClp0Hd"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(texts=train_x, targets=train_y, vocab=vocab)\n",
        "valid_dataset = TextClassificationDataset(texts=valid_x, targets=valid_y, vocab=vocab)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVpxSMlVp0Hd"
      },
      "outputs": [],
      "source": [
        "for x, y in train_loader:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDskKhd-p0He"
      },
      "outputs": [],
      "source": [
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD96riGMp0He"
      },
      "source": [
        "### How can we define layers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlOx4noop0He"
      },
      "outputs": [],
      "source": [
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So1HsX_qp0He"
      },
      "outputs": [],
      "source": [
        "embedding_layer = nn.Embedding(num_embeddings=len(vocab),\n",
        "                               embedding_dim=embeddings.shape[-1],\n",
        "                               padding_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32CjrOlep0He"
      },
      "outputs": [],
      "source": [
        "x_embed = embedding_layer(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7PJwiS9p0He"
      },
      "outputs": [],
      "source": [
        "x_embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Q2-JF08p0He"
      },
      "outputs": [],
      "source": [
        "x_embed.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e1pd_Qep0He"
      },
      "source": [
        "### But we have read our embedding matrix\n",
        "Thus, it is initialized with pretrained weights.\n",
        "With such initialization, by default it is frozen, inside ```.from_pretrained(embeddings, padding_idx=0)``` there is a flag ```freeze```, which is responsible for freezing the weights if necessity. That is, these weights will not be updated during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znxoHgLVp0He"
      },
      "outputs": [],
      "source": [
        "embeddings = torch.tensor(embeddings).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiiLUx0rp0He"
      },
      "outputs": [],
      "source": [
        "embedding_layer = nn.Embedding.from_pretrained(embeddings, padding_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQFxez0Xp0He"
      },
      "outputs": [],
      "source": [
        "x_embed = embedding_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaGyaaDsp0Hf"
      },
      "source": [
        "### A bit of LSTM\n",
        "Below will be about ```batch_first=True```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acLF3I9Ip0Hf"
      },
      "outputs": [],
      "source": [
        "lstm = nn.LSTM(input_size=300, hidden_size=128, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0GMGrzrp0Hf"
      },
      "outputs": [],
      "source": [
        "x_lstm, _ = lstm(x_embed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9T9Gui8-p0Hf"
      },
      "outputs": [],
      "source": [
        "# 256 because it is a concatenation of the LSTM that read the text from left to right\n",
        "# and the LSTM that read the text from right to left\n",
        "x_lstm.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKr1jf2hp0Hf"
      },
      "outputs": [],
      "source": [
        "# got rid of the time dimension\n",
        "x_lstm.mean(dim=1).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6paVGlRp0Hf"
      },
      "source": [
        "### Let's create our own network\n",
        "There is more detailed information about why we use classes at the end of the the first homework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMWU-Ykjp0Hf"
      },
      "outputs": [],
      "source": [
        "class DeepAverageNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, embeddings, linear_1_size, linear_2_size, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings, padding_idx=0)\n",
        "\n",
        "        self.batch_norm = nn.BatchNorm1d(num_features=embeddings.shape[-1])\n",
        "\n",
        "        self.linear_1 = nn.Linear(in_features=embeddings.shape[-1], out_features=linear_1_size)\n",
        "        self.linear_2 = nn.Linear(in_features=linear_1_size, out_features=linear_2_size)\n",
        "        self.linear_3 = nn.Linear(in_features=linear_2_size, out_features=n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # translating word indices into embeddings of these words\n",
        "        # (batch_size, sequence_length) -> (batch_size, sequence_length, embedding_dim)\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "        # aggregating our embeddings by time dimension\n",
        "        # (batch_size, sequence_length, embedding_dim) -> (batch_size, embedding_dim)\n",
        "        x = x.sum(dim=1)\n",
        "\n",
        "        # normalization\n",
        "        # (batch_size, embedding_dim) -> (batch_size, embedding_dim)\n",
        "        x = self.batch_norm(x)\n",
        "\n",
        "        # passing through the first linear layer\n",
        "        # (batch_size, embedding_dim) -> (batch_size, linear_1_size)\n",
        "        x = self.linear_1(x)\n",
        "\n",
        "        # applying nonlinearity\n",
        "        # (batch_size, linear_1_size) -> (batch_size, linear_1_size)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        # passing through the second linear layer\n",
        "        # (batch_size, linear_1_size) -> (batch_size, linear_2_size)\n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        # applying nonlinearity\n",
        "        # (batch_size, linear_2_size) -> (batch_size, linear_2_size)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        # converting into the number of classes using a linear transformation\n",
        "        # (batch_size, linear_2_size) -> (batch_size, n_classes)\n",
        "        x = self.linear_3(x)\n",
        "\n",
        "        ## in theory there should have been a softmax here\n",
        "        ## but we will use the nn.CrossEntropyLoss() loss\n",
        "        ## its documentation says\n",
        "        ## This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.\n",
        "        ## this is some optimization that includes both the softmax and the negative log likelihood loss itself\n",
        "        ## since we have a softmax in the loss, we will not use it in the net\n",
        "        ## at the prediction stage (not training) we will separately do the softmax to obtain the class distribution\n",
        "        ##\n",
        "        ## (batch_size, n_classes) -> (batch_size, n_classes)\n",
        "        # x = torch.softmax(x, dim=-1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX-__8jBp0Hf"
      },
      "outputs": [],
      "source": [
        "model = DeepAverageNetwork(embeddings=embeddings,\n",
        "                           linear_1_size=256,\n",
        "                           linear_2_size=128,\n",
        "                           n_classes=len(category2index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8cH1BsKp0Hf"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# set the optimizer\n",
        "# optimizer = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8s5hevKp0Hf"
      },
      "source": [
        "### Write a training loop\n",
        "What it should include:\n",
        "1. Obtaining model predictions\n",
        "1. Calculating the loss function\n",
        "1. Calculating gradients\n",
        "1. Gradient descent step\n",
        "1. Zeroing of the gradients\n",
        "1. Saving the loss value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWZLu-sCp0Hf"
      },
      "outputs": [],
      "source": [
        "losses = list()\n",
        "\n",
        "# in model training we have a situation where some layers behave differently at the training and prediction stages\n",
        "# for example, batch norm (as well as all other normalizations) and dropout\n",
        "# this puts the model in the training mode\n",
        "model.train()\n",
        "\n",
        "for x, y in train_loader:\n",
        "\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBcm4fGcp0Hg"
      },
      "source": [
        "### Write a validation loop\n",
        "What it should include:\n",
        "1. Getting model predictions\n",
        "1. Calculating the loss function\n",
        "1. Saving the loss value\n",
        "\n",
        "Also, using the context ```with torch.no_grad():```, you can explicitly tell torch not to save the necessary parameters for calculating gradients. Required for the prediction mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUFiuWi3p0Hg"
      },
      "outputs": [],
      "source": [
        "losses = list()\n",
        "\n",
        "# this puts the model in the prediction mode\n",
        "# that is, batch norm statistics are recorded, dropout does not throw the features out\n",
        "model.eval()\n",
        "\n",
        "# note that we have changed our loader to the validation one\n",
        "for x, y in valid_loader:\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # getting model predictions\n",
        "        # loss calculation\n",
        "        ...\n",
        "\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNia-Epvp0Hg"
      },
      "source": [
        "### Train for several epochs\n",
        "One epoch is one pass through the dataset.\n",
        "Steps:\n",
        "- Change something in the model, add a dropout, etc.\n",
        "- Stop training with early stopping\n",
        "- Add metric calculation during training and prediction (e.g. micro F1). To do this, you can, for example, save the model's predictions\n",
        "- After training, draw how the loss function changes on the training and validation dataset as training progresses, how the metrics change\n",
        "- Optional: build a confusion matrix\n",
        "\n",
        "Hints:\n",
        "- To save predictions correctly, you need to detach the variable from the graph, that is, do ```x.detach()```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pXb39Zbp0Hg"
      },
      "outputs": [],
      "source": [
        "for n_epoch in range(2):\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU6xf7X1p0Hg"
      },
      "source": [
        "### Important and not so intuitive points about LSTM in Torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v5Qyhl7p0Hg"
      },
      "source": [
        "By default, LSTM accepts data with the following dimensions:\n",
        "```python\n",
        "(seq_len, batch, input_size)\n",
        "```\n",
        "This is done for the purpose of optimization at a lower level.\n",
        "\n",
        "We operate with the following objects:\n",
        "```python\n",
        "(batch, seq_len, input_size)\n",
        "```\n",
        "For the LSTM to work correctly, we can either pass the parameter ```batch_first=True``` during layer initialization,\n",
        "or transpose (change) the first and second dimensions of our x before feeding it to the layer.\n",
        "[More on LSTM](https://pytorch.org/docs/stable/nn.html#lstm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qmjjPAop0Hg"
      },
      "source": [
        "- 128 - batch size\n",
        "- 64 - sequence length (number of words)\n",
        "- 1024 - word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXZ_hDWrp0Hg"
      },
      "outputs": [],
      "source": [
        "x = torch.rand(128, 64, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3vDLe4mp0Hg"
      },
      "outputs": [],
      "source": [
        "# first way\n",
        "lstm = torch.nn.LSTM(1024, 512, batch_first=True)\n",
        "\n",
        "pred, mem = lstm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o27w6UVmp0Hg"
      },
      "outputs": [],
      "source": [
        "pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5ETAZjOp0Hg"
      },
      "outputs": [],
      "source": [
        "# second way\n",
        "lstm = torch.nn.LSTM(1024, 512)\n",
        "\n",
        "# swap the dimensions of batch and seq_len\n",
        "x_transposed = x.transpose(0, 1)\n",
        "pred_transposed, mem = lstm(x_transposed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22KC2zxHp0Hg"
      },
      "outputs": [],
      "source": [
        "# we still have the (seq_len, batch, input_size) dimensions\n",
        "pred_transposed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LubgqD-Gp0Hg"
      },
      "outputs": [],
      "source": [
        "# just transpose again\n",
        "pred = pred_transposed.transpose(0, 1)\n",
        "pred.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McAoWwrJp0Hh"
      },
      "source": [
        "### Another important point about LSTM\n",
        "\n",
        "The input can also be a packed variable length sequence. See [torch.nn.utils.rnn.pack_padded_sequence()](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) or [torch.nn.utils.rnn.pack_sequence()](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence) for details.\n",
        "\n",
        "This is an internal Torch design that allows you to not read the ```PAD``` token, but still work with batches. That is, inside the batch we can pass to the LSTM that we have variable-length data. Don't forget that [torch.nn.utils.rnn.PackedSequence](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.PackedSequence) is given to the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QLLEloEp0Hh"
      },
      "source": [
        "## Homework\n",
        "\n",
        "1. Create a neural network class, add the necessary operations, the architecture is described below\n",
        "1. Write the training procedure (summarize what was discussed above)\n",
        "1. Add logging\n",
        "    1. Save the loss at each training iteration __0.25 points__\n",
        "    1. Save the loss of the train and test each epoch __0.25 points__\n",
        "    1. Calculate metrics at each epoch __0.25 points__\n",
        "    1. Add a progress bar that shows the average loss of the last 500 iterations __0.25 points__\n",
        "1. Add early stopping __0.5 points__\n",
        "1. Draw graphs of loss, metrics, conjugation matrix __0.5 points__\n",
        "\n",
        "### Architecture (what to try)\n",
        "1. Pre-trained embeddings. Read [here](https://pytorch.org/docs/stable/nn.html#embedding) (from_pretrained) how to add your own embeddings, above we read the embedding matrix. __0 points__\n",
        "1. Retrain the embeddings together with the network and with a different learning rate (specified in the optimizer). __2 points__\n",
        "1. Bidirectional LSTM. __1 point__\n",
        "1. Write the correct mean/max pooling, which does not take into account paddings, or rather masks them. __2 points__\n",
        "1. Add [torch.nn.utils.rnn.pack_padded_sequence()](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence) and [torch.nn.utils.rnn.pack_sequence()](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_sequence) for LSTM. Info [here](#Another-important-point-about-LSTM) __2 points__\n",
        "1. Add spatial dropout for LSTM input (not just a standard item when initializing LSTM) __1 point__\n",
        "1. Add BatchNorm/LayerNorm/Dropout/Residual/etc __2 points__\n",
        "1. Add scheduler __1 point__\n",
        "1. Train on GPU __2 points__\n",
        "1. your madness\n",
        "\n",
        "## Grade: 10 points maximum\n",
        "\n",
        "# Write down the results of the experiments\n",
        "# What worked and what didn't and why\n",
        "# And conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBkj5zEfp0Hh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}